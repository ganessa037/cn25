[tool:pytest]
# Integration Test Configuration for Document Parser

# Test discovery
testpaths = .
python_files = test_*.py
python_classes = Test*
python_functions = test_*

# Test execution options
addopts = 
    --strict-markers
    --strict-config
    --verbose
    --tb=short
    --showlocals
    --durations=10
    --cov=src/document_parser
    --cov-report=html:reports/coverage_html
    --cov-report=xml:reports/coverage.xml
    --cov-report=term-missing
    --cov-fail-under=70
    --junitxml=reports/junit.xml
    --html=reports/integration_report.html
    --self-contained-html
    --capture=no
    --log-cli=true
    --log-cli-level=INFO
    --log-cli-format=%(asctime)s [%(levelname)8s] %(name)s: %(message)s
    --log-cli-date-format=%Y-%m-%d %H:%M:%S
    --disable-warnings
    --timeout=300
    --timeout-method=thread

# Parallel execution
# Uncomment to enable parallel execution (requires pytest-xdist)
# -n auto

# Test markers
markers =
    integration: Integration tests
    api: API endpoint tests
    performance: Performance and load tests
    slow: Slow running tests (>30 seconds)
    fast: Fast running tests (<5 seconds)
    smoke: Smoke tests for basic functionality
    end_to_end: End-to-end pipeline tests
    load: Load testing scenarios
    stress: Stress testing scenarios
    memory: Memory usage tests
    concurrent: Concurrent execution tests
    network: Tests requiring network access
    external: Tests requiring external services
    regression: Regression tests
    critical: Critical functionality tests
    edge_case: Edge case testing
    error_handling: Error handling tests
    validation: Data validation tests
    security: Security-related tests

# Minimum version requirements
minversion = 6.0

# Test timeout settings
timeout = 300
timeout_method = thread

# Coverage settings
[coverage:run]
source = src/document_parser
omit = 
    */tests/*
    */test_*
    */__pycache__/*
    */venv/*
    */env/*
    */.venv/*
    */site-packages/*
    */conftest.py
    */setup.py
    */migrations/*
    */static/*
    */media/*
    */locale/*
    */docs/*
    */build/*
    */dist/*
    */.git/*
    */.pytest_cache/*
    */.coverage
    */htmlcov/*
    */node_modules/*

[coverage:report]
# Regexes for lines to exclude from consideration
exclude_lines =
    # Have to re-enable the standard pragma
    pragma: no cover
    
    # Don't complain about missing debug-only code:
    def __repr__
    if self\.debug
    
    # Don't complain if tests don't hit defensive assertion code:
    raise AssertionError
    raise NotImplementedError
    
    # Don't complain if non-runnable code isn't run:
    if 0:
    if __name__ == .__main__.:
    
    # Don't complain about abstract methods
    @(abc\.)?abstractmethod
    
    # Don't complain about type checking imports
    if TYPE_CHECKING:
    
    # Don't complain about platform specific code
    if sys.platform
    
    # Don't complain about deprecated code
    @deprecated
    warnings.warn

ignore_errors = True
show_missing = True
skip_covered = False
skip_empty = False
sort = Cover
precision = 2

[coverage:html]
directory = reports/coverage_html
title = Document Parser Integration Test Coverage

[coverage:xml]
output = reports/coverage.xml

# Logging configuration
[tool:pytest.logging]
log_auto_indent = true
log_cli = true
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] %(name)s: %(message)s (%(filename)s:%(lineno)d)
log_cli_date_format = %Y-%m-%d %H:%M:%S
log_file = reports/integration_tests.log
log_file_level = DEBUG
log_file_format = %(asctime)s [%(levelname)8s] %(name)s: %(message)s (%(filename)s:%(lineno)d)
log_file_date_format = %Y-%m-%d %H:%M:%S

# Filter warnings
filterwarnings =
    ignore::UserWarning
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning
    ignore::ImportWarning
    ignore::ResourceWarning
    # Specific warnings to ignore
    ignore:.*deprecated.*:DeprecationWarning
    ignore:.*unclosed.*:ResourceWarning
    ignore:.*invalid escape sequence.*:DeprecationWarning
    # TensorFlow warnings
    ignore:.*tensorflow.*:UserWarning
    # OpenCV warnings
    ignore:.*cv2.*:UserWarning
    # Tesseract warnings
    ignore:.*tesseract.*:UserWarning
    # EasyOCR warnings
    ignore:.*easyocr.*:UserWarning

# Test collection
collect_ignore = [
    "setup.py",
    "conftest.py",
    "__pycache__",
    ".git",
    ".pytest_cache",
    "node_modules",
    "venv",
    "env",
    ".venv",
    "build",
    "dist",
    "docs",
    "static",
    "media",
    "locale",
    "migrations"
]

# Test discovery patterns
python_files = test_*.py *_test.py
python_classes = Test* *Tests
python_functions = test_*

# Console output
console_output_style = progress

# Test session
usefixtures = 
    # Add global fixtures here if needed

# Performance settings
# Adjust based on system capabilities
max_worker_restart = 0
worker_timeout = 300

# Memory management
# Enable garbage collection between tests
addopts = --forked

# Test data
tmp_path_retention_count = 3
tmp_path_retention_policy = failed

# Debugging
# Uncomment for debugging
# --pdb
# --pdbcls=IPython.terminal.debugger:Pdb
# --capture=no

# Documentation
doctest_optionflags = NORMALIZE_WHITESPACE IGNORE_EXCEPTION_DETAIL ELLIPSIS
doctest_ignore_import_errors = true

# Test ordering
# Uncomment to enable test ordering (requires pytest-ordering)
# --order-dependencies
# --order-group-scope=session

# Benchmark settings (if using pytest-benchmark)
# benchmark-only = false
# benchmark-sort = mean
# benchmark-group-by = group
# benchmark-disable = false
# benchmark-skip = false
# benchmark-warmup = true
# benchmark-warmup-iterations = 100000
# benchmark-disable-gc = false
# benchmark-min-rounds = 5
# benchmark-max-time = 1.0
# benchmark-min-time = 0.000005
# benchmark-timer = time.perf_counter

# Flaky test settings (if using pytest-flaky)
# flaky-report = true
# force-flaky = false

# Retry settings (if using pytest-rerunfailures)
# reruns = 2
# reruns-delay = 1

# Test selection
# Run only tests that match the given substring expression
# -k "test_api or test_performance"

# Exit on first failure
# -x

# Exit after N failures
# --maxfail=5

# Show extra test summary info
# -r fEsxXvs

# Quiet mode
# -q

# Very verbose mode
# -vv

# Show local variables in tracebacks
# -l

# Show full diff for assert failures
# --tb=long

# No header
# --no-header

# No summary
# --no-summary

# Collect only (don't run tests)
# --collect-only

# Dry run
# --setup-only

# Show fixtures
# --fixtures

# Show available markers
# --markers

# Show pytest version
# --version

# Show help
# --help